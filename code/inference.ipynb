{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2, random, argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "#get also f1    \n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "from helpers import corresponding_label_to_video, get_rtf_text\n",
    "from model import EventClassifier\n",
    "from collections import deque\n",
    "NUM_FRAMES_INPUT = 16  # Number of frames to sample from each video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\2025_ICIAP_FIRE\\onfire_env\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Model total parameters: 124739328 trainable parameters: 297216\n",
      "Video Encoder total parameters: 86227200 trainable parameters: 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class_to_idx = {'No event': 0, 'Fire': 1, 'Smoke': 2}\n",
    "model = EventClassifier(num_labels=len(class_to_idx)).to(device)\n",
    "TYPE  = 'baseline_aug_diff'  # Change to 'train' or 'test' as needed\n",
    "ROOT_DIR = f'E:/2025_ICIAP_FIRE/output/{TYPE}'\n",
    "OUTPUT_DIR = f'E:/2025_ICIAP_FIRE/output/{TYPE}/results'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "PATH_TO_MODEL = os.path.join(ROOT_DIR, 'event_classifier.pth')\n",
    "\n",
    "model.load_state_dict(torch.load(PATH_TO_MODEL, map_location=device))\n",
    "CONTROLLER_FRAMES = 4\n",
    "MAX_FRAMES = 20000  # Maximum number of frames to process per video\n",
    "PATH_TO_CSV = os.path.join(OUTPUT_DIR, f'results_{CONTROLLER_FRAMES}.csv')\n",
    "header = ['video_name', 'current_second', 'No_event', 'Fire', 'Smoke']\n",
    "df = pd.DataFrame(columns=header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split(videos_path, labels_path, seed, split_ratio=0.8):\n",
    "    vids, lbls = corresponding_label_to_video(videos_path, labels_path)\n",
    "    combined = list(zip(vids, lbls))\n",
    "    random.seed(seed); random.shuffle(combined)\n",
    "    vids, lbls = zip(*combined)\n",
    "    idx = int(len(vids) * split_ratio)\n",
    "    return vids[:idx], lbls[:idx], vids[idx:], lbls[idx:]\n",
    "\n",
    "\n",
    "def sample_or_pad(frames):\n",
    "    \"\"\" Return exactly NUM_FRAMES_INPUT frames: random sample or pad with repeats/zeros. \"\"\"\n",
    "    if len(frames) >= NUM_FRAMES_INPUT:\n",
    "        return random.sample(frames, NUM_FRAMES_INPUT)\n",
    "    pad_count = NUM_FRAMES_INPUT - len(frames)\n",
    "    if frames:\n",
    "        return frames + [frames[-1]] * pad_count\n",
    "    return [np.zeros((224,224,3), dtype=np.uint8)] * NUM_FRAMES_INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 353 video files and 353 label files.\n"
     ]
    }
   ],
   "source": [
    "path_to_videos = 'E:/2025_ICIAP_FIRE/dataset'\n",
    "path_to_labels = 'E:/2025_ICIAP_FIRE/GT'\n",
    "train_v, train_l, val_v, val_l = load_and_split(path_to_videos, path_to_labels, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_videos, val_labels, class_to_idx, fps_out, device,df):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for vid, lbl in tqdm(zip(val_videos, val_labels), total=len(val_videos), desc=\"Validating\"):\n",
    "            current_row = {'video_name': vid, 'current_second': [], 'No_event': [], 'Fire': [], 'Smoke': []}\n",
    "            cap = cv2.VideoCapture(vid)\n",
    "\n",
    "\n",
    "            if not cap.isOpened(): continue\n",
    "\n",
    "            timestart, cls_event = get_rtf_text(lbl)\n",
    "\n",
    "            \n",
    "         \n",
    "            base_idx = []\n",
    "            for cls in cls_event:\n",
    "                if cls in class_to_idx:\n",
    "                    base_idx.append(class_to_idx[cls])\n",
    "                else:\n",
    "                    print(f\"Warning: Class '{cls}' not found in class_to_idx. Using 'No event' instead.\")\n",
    "                    base_idx.append(class_to_idx[\"No event\"])\n",
    "\n",
    "            frame_buf, prev_kvs = [], None\n",
    "            idx_in, idx_out = -1, -1\n",
    "            fps_in = cap.get(cv2.CAP_PROP_FPS)\n",
    "            fcount = 0\n",
    "\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if fcount >= MAX_FRAMES: \n",
    "                    print(f\"Reached maximum frame count ({MAX_FRAMES}) for video {vid}. Stopping.\")\n",
    "                    break\n",
    "                if not ret: break\n",
    "                idx_in += 1; fcount += 1\n",
    "                out_due = int(idx_in / fps_in * fps_out)\n",
    "                if out_due > idx_out:\n",
    "                    idx_out += 1\n",
    "                    frame_buf.append(frame)\n",
    "\n",
    "                if len(frame_buf) >= NUM_FRAMES_INPUT:\n",
    "                    batch = sample_or_pad(frame_buf)\n",
    "                    frame_buf = []\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                cls_idx = base_idx\n",
    "                if (fcount / fps_in) < timestart:\n",
    "                    cls_idx = class_to_idx[\"No event\"]\n",
    "                gt_vector = [0] * len(class_to_idx)\n",
    "                if isinstance(cls_idx, int):\n",
    "                    cls_idx = [cls_idx]\n",
    "                for label in cls_idx:\n",
    "                    gt_vector[label] = 1\n",
    "                batch = [cv2.resize(f, (224,224)) for f in batch]\n",
    "                preds, prev_kvs = model(batch, old_past_key_values=prev_kvs)\n",
    "                # Append the sigmoid output\n",
    "                probs = torch.sigmoid(preds)\n",
    "                raw_preds = preds\n",
    "                preds = (probs > 0.5).int().cpu().tolist()\n",
    "                #if preds is a single value, make it a list and convert to one-hot encoding\n",
    "                if isinstance(preds, int):\n",
    "                    empty_vector = [0] * len(class_to_idx)\n",
    "                    empty_vector[preds] = 1\n",
    "                    preds = empty_vector\n",
    "                    #rouding the current second to the nearest second\n",
    "                current_row['current_second']= int(round(fcount / fps_in, 0))\n",
    "                preds= preds[0]\n",
    "                current_row['No_event'] = preds[0]\n",
    "                current_row['Fire']= preds[1]\n",
    "                current_row['Smoke']= preds[2]\n",
    "                df = pd.concat([df, pd.DataFrame([current_row])], ignore_index=True)\n",
    "                #save the current dataframe to csv\n",
    "                df.to_csv(PATH_TO_CSV, index=False)\n",
    "                \n",
    "            # Handle any remaining frames in the buffer\n",
    "\n",
    "            if frame_buf:\n",
    "                batch = sample_or_pad(frame_buf)\n",
    "                cls_idx = base_idx\n",
    "                if (fcount / fps_in) < timestart:\n",
    "                    cls_idx = class_to_idx[\"No event\"]\n",
    "                gt_vector = [0] * len(class_to_idx)\n",
    "                if isinstance(cls_idx, int):\n",
    "                    cls_idx = [cls_idx]\n",
    "                for label in cls_idx:\n",
    "                    gt_vector[label] = 1\n",
    "                batch = [cv2.resize(f, (224,224)) for f in batch]\n",
    "                preds, prev_kvs = model(batch, old_past_key_values=prev_kvs)\n",
    "                # Append the sigmoid output\n",
    "                probs = torch.sigmoid(preds)\n",
    "                raw_preds = preds\n",
    "                preds = (probs > 0.5).int().cpu().tolist()\n",
    "                \n",
    "                #if preds is a single value, make it a list and convert to one-hot encoding\n",
    "                if isinstance(preds, int):\n",
    "                    empty_vector = [0] * len(class_to_idx)\n",
    "                    empty_vector[preds] = 1\n",
    "                preds= preds[0]\n",
    "                current_row['current_second'] = int(round(fcount / fps_in, 0))\n",
    "                current_row['No_event'] = preds[0]\n",
    "                current_row['Fire'] = preds[1]\n",
    "                current_row['Smoke'] = preds[2]\n",
    "                df = pd.concat([df, pd.DataFrame([current_row])], ignore_index=True)\n",
    "                #save the current dataframe to csv\n",
    "                df.to_csv(PATH_TO_CSV, index=False)\n",
    "            cap.release()\n",
    "    return df\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   4%|▍         | 3/71 [00:41<15:37, 13.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached maximum frame count (20000) for video E:/2025_ICIAP_FIRE/dataset\\1\\Video192.mp4. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 71/71 [09:24<00:00,  7.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to E:/2025_ICIAP_FIRE/output/baseline_aug_diff/results\\results_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "df = evaluate(model, val_v, val_l, class_to_idx, CONTROLLER_FRAMES, device, df)\n",
    "df.to_csv(PATH_TO_CSV, index=False)\n",
    "print(f\"Results saved to {PATH_TO_CSV}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onfire_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
